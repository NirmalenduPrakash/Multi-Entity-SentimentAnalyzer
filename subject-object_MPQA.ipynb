{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob2\n",
    "import os\n",
    "import bs4\n",
    "import nltk\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Users/nirmalenduprakash/Documents/Project/NLP/Sentiment Mining')\n",
    "filenames=glob2.glob('database.mpqa.3.0/gate_anns/**/*.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "tokenizer = BertTokenizer.from_pretrained(os.curdir)\n",
    "seq_train=[]\n",
    "for file in filenames:\n",
    "    with open(file,'r') as f:\n",
    "        soup=bs4.BeautifulSoup(f)\n",
    "        txt=soup.select('TextWithNodes')[0].text\n",
    "        sents=getSentences(txt)\n",
    "        txt_with_nodes=str(soup.select('TextWithNodes')[0])    \n",
    "        agents=[{'startNode':node['startnode'],\\\n",
    "                       'endNode':node['endnode'], \\\n",
    "                       'text':getNodeText(txt_with_nodes,node['startnode'],node['endnode'])}\\\n",
    "                      for node in soup.select(\"Annotation[Type=agent]\")]\n",
    "        sentiments=[{'startNode':node['startnode'],\\\n",
    "                       'endNode':node['endnode'], \\\n",
    "                       'intensity':str(list(list(node.children)[3])[3]).split('>')[1].split('<')[0],\\\n",
    "                       'type':str(list(list(node.children)[5])[3]).split('>')[1].split('<')[0]}\\\n",
    "                      for node in soup.select(\"Annotation[Type=attitude]\")]\n",
    "        targets=[{'startNode':node['startnode'],\\\n",
    "                       'endNode':node['endnode'], \\\n",
    "                       'text':getNodeText(txt_with_nodes,node['startnode'],node['endnode'])}\\\n",
    "                      for node in soup.select(\"Annotation[Type=sTarget]\")]\n",
    "        for indx,sent in enumerate(sents):\n",
    "            nodeText=getNodeSent(sent,txt_with_nodes)        \n",
    "            nodes_in_txt=re.findall('\\d+',nodeText)\n",
    "            sent_agent=None\n",
    "            sent_target=None\n",
    "            sent_sentiment_type=None\n",
    "            sent_sentiment_intensity=None\n",
    "            \n",
    "            for node in nodes_in_txt:\n",
    "                sent_agents=[itm['text'] for itm in agents if itm['startNode']==node]\n",
    "                if(len(sent_agents)>0):\n",
    "                    sent_agent=sent_agents[0].replace('  ',' ')\n",
    "                sent_targets=[itm['text'] for itm in targets if itm['startNode']==node]\n",
    "                if(len(sent_targets)>0):\n",
    "                    sent_target=sent_targets[0].replace('  ',' ') \n",
    "                sent_intensities=[itm['intensity'] for itm in sentiments if itm['startNode']==node]\n",
    "                if(len(sent_intensities)>0):\n",
    "                    sent_sentiment_intensity=sent_intensities[0]\n",
    "                sent_types=[itm['type'] for itm in sentiments if itm['startNode']==node]\n",
    "                if(len(sent_types)>0):\n",
    "                    sent_sentiment_type=sent_types[0]     \n",
    "#             print('sent:{}'.format(sent),'agent:{}'.format(sent_agent),'target:{}'.format(sent_target),\\\n",
    "#                   'sentiment type:{}'.format(sent_sentiment_type),\\\n",
    "#                   'sentiment intensity:{}'.format(sent_sentiment_intensity))\n",
    "            \n",
    "            txt_with_nodes=txt_with_nodes[len(nodeText):]\n",
    "            labels=tokenizer.tokenize(sent)\n",
    "            sent=tokenizer.tokenize(sent)\n",
    "            \n",
    "            if(sent_agent is not None):\n",
    "                n=len(tokenizer.tokenize(sent_agent))\n",
    "                gms=nltk.ngrams(labels,n)\n",
    "                for i,g in enumerate(gms):\n",
    "                    if(' '.join(g).strip()==' '.join(tokenizer.tokenize(sent_agent)).strip()):\n",
    "                        labels[i:i+len(g)]=['B-AG' if j==0 else 'I-AG' for j in range(len(g))]\n",
    "                        break\n",
    "                        \n",
    "            if(sent_target is not None and (sent_agent is not None and sent_agent.strip()!=sent_target.strip())):\n",
    "                n=len(tokenizer.tokenize(sent_target))\n",
    "                gms=nltk.ngrams(labels,n)\n",
    "                for i,g in enumerate(gms):\n",
    "                    if(' '.join(g).strip()==' '.join(tokenizer.tokenize(sent_target)).strip()):\n",
    "                        labels[i:i+len(g)]=['B-TG' if j==0 else 'I-TG' for j in range(len(g))]\n",
    "                        break            \n",
    "                 \n",
    "            labels=['O' if w not in ['B-AG','I-AG','B-TG','I-TG'] else w for w in labels]  \n",
    "            seq_train.append({'sent':sent,'labels':labels})\n",
    "#             print(seq_train)\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNodeText(text,node1,node2):\n",
    "    text= text[text.index(\"\\\"\"+node1+\"\\\"\"):text.index(\"\\\"\"+node2+\"\\\"\")]\n",
    "    nodes_in_text=re.findall(\"\\\"\\d*\\\"\",text)\n",
    "    for node in nodes_in_text:\n",
    "        text=text.replace(node,'')    \n",
    "    text=text.replace('node','')\\\n",
    "              .replace('>','').replace('<','').replace('id','').replace('=','').replace('\\\"','')\\\n",
    "             .replace('/','')#.replace(node1,'')#.replace(node2,'')\n",
    "\n",
    "    return text    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSentences(text):\n",
    "    return nltk.sent_tokenize(text)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNodeSent(sent,txt_with_nodes):\n",
    "#     print(sent,txt_with_nodes)\n",
    "    result=''\n",
    "    search_text=txt_with_nodes\n",
    "    for word in nltk.word_tokenize(sent):  \n",
    "        try:\n",
    "            result=result+str(search_text[:search_text.index(word) + len(word)])      \n",
    "            search_text=txt_with_nodes[len(result):]\n",
    "        except:\n",
    "            continue        \n",
    "    return result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Sino-US plane collision event that occurred on 1 April can best illustrate the US alternate application of the \"two-isms\" in its diplomacy.'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getNodeSent(sents[18],str(soup.select('TextWithNodes')[0]))\n",
    "# txt_with_nodes\n",
    "sents[14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst=[('Following', 'the', 'end'), ('the', 'end', 'of'), ('end', 'of', 'the'), ('of', 'the', 'Cold'), ('the', 'Cold', 'War,'), ('Cold', 'War,', 'the'), ('War,', 'the', 'United'), ('the', 'United', 'States'), ('United', 'States', 'has'), ('States', 'has', 'become'), ('has', 'become', 'the'), ('become', 'the', \"world's\"), ('the', \"world's\", 'sole'), (\"world's\", 'sole', 'superpower'), ('sole', 'superpower', 'and'), ('superpower', 'and', 'the'), ('and', 'the', 'trend'), ('the', 'trend', 'of'), ('trend', 'of', 'pragmatism'), ('of', 'pragmatism', 'in'), ('pragmatism', 'in', 'its'), ('in', 'its', 'foreign'), ('its', 'foreign', 'policy'), ('foreign', 'policy', 'has'), ('policy', 'has', 'become'), ('has', 'become', 'clearer'), ('become', 'clearer', 'and'), ('clearer', 'and', 'clearer.')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.DataFrame(seq_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('data.pkl','wb') as f:\n",
    "    pickle.dump(df,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "max_len_lbl=np.max([df['labels'].apply(lambda x: len(x))])\n",
    "max_len_sent=np.max([df['sent'].apply(lambda x: len(x))])\n",
    "# df['sent']=df['sent'].apply(lambda x:['[CLS]'+x+'[SEP]'])\n",
    "# df['labels']=df['labels'].apply(lambda x:['[CLS]'+x+'[SEP]'])\n",
    "max_len=max(max_len_lbl,max_len_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(tokens,max_len,cls=False):\n",
    "  if(len(tokens)<max_len):\n",
    "    if(cls):\n",
    "        return ['[CLS]']+tokens+['[PAD]' for _ in range(max_len-len(tokens)-1)]\n",
    "    else:\n",
    "        return tokens+['[PAD]' for _ in range(max_len-len(tokens))]\n",
    "  else:\n",
    "    return tokens[:max_len] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['labels']=df['labels'].apply(lambda x: pad([l for l in x],max_len))\n",
    "df['sent']=df['sent'].apply(lambda x: pad([l for l in x],max_len,True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['encoding']=df['sent'].apply(lambda x: tokenizer.convert_tokens_to_ids(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['attn_mask']=df['encoding'].apply(lambda x: [0 if tok==str('[PAD]') else 1 for tok in x ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "with open('/Users/nirmalenduprakash/Documents/Project/NLP/classification/val_losses.pkl','rb') as f:\n",
    "    losses=pickle.load(f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.iloc[620]['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('DEEPML': conda)",
   "language": "python",
   "name": "python37664bitdeepmlcondaa8309bcee70a4550b3efd0d3a9e3abda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
