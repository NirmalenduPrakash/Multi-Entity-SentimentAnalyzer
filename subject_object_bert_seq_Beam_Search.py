# -*- coding: utf-8 -*-
"""Subject-Object-BERT-SEQ.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14V0C955tnh4qmUoA9sgOuBMXG2AaZBQ5
"""

import pandas as pd
import torch
from transformers import BertTokenizer
from torch.nn import functional as F
import numpy

from transformers import BertModel
bert_model=BertModel.from_pretrained('/home/svu/e0401988/NLP/classification')
tokenizer = BertTokenizer.from_pretrained('/home/svu/e0401988/NLP/classification')

import pickle
with open('/home/svu/e0401988/NLP/BeamSearch/data.pkl','rb') as f:
  df=pickle.load(f)

MAX_LENGTH=len(df.iloc[0]['encoding'])
tag_index={'[PAD]':0,'<s>':1,'B-AG':2,'I-AG':3,'B-TG':4,'I-TG':5,'O':6,'</s>':7}
df['labels']=df['labels'].apply(lambda x: [tag_index[lbl] for lbl in x])

from torch.utils.data import Dataset
class SequenceDataset(Dataset):
  def __init__(self,df):
    self.df=df

  def __len__(self):
    return len(self.df)

  def __getitem__(self,index):
    return torch.tensor(self.df.iloc[index]['encoding']),torch.tensor(self.df.iloc[index]['attn_mask']),torch.tensor(self.df.iloc[index]['labels'])

from torch.utils.data import DataLoader
import numpy as np
msk = np.random.rand(len(df)) < 0.8
train=df[msk]
val=df[~msk]
train_set=SequenceDataset(train)
val_set=SequenceDataset(val)
train_loader=DataLoader(train_set, batch_size = 2)
val_loader = DataLoader(val_set, batch_size = 2)

import torch.nn as nn
class Encoder(nn.Module):
    def __init__(self, freeze_bert = True):
        super(Encoder, self).__init__()
        self.bert_layer = bert_model#BertModel.from_pretrained('bert-base-uncased')

        for p in self.bert_layer.parameters():
            p.requires_grad = False

    def forward(self, seq, attn_masks):
        cont_reps, _ = self.bert_layer(seq, attention_mask = attn_masks)
        return cont_reps[:,0]

class Decoder(nn.Module):
    def __init__(self,vocab_size, hidden_size,output_size, dropout_p=0.1):
        super(Decoder, self).__init__()
        self.hidden_size = hidden_size
        self.vocab_size=vocab_size
        self.output_size = output_size
        self.dropout = nn.Dropout(0.1)
        self.embedding = nn.Embedding(self.vocab_size,self.hidden_size)
        self.gru = nn.GRU(self.hidden_size, self.hidden_size,batch_first=True)
        self.out = nn.Linear(self.hidden_size, self.output_size)

    def forward(self, hidden,input):
       embedded = self.embedding(input).view(hidden.shape[0], 1, -1)
       embedded = self.dropout(embedded)
       output, hidden = self.gru(embedded, hidden.permute(1,0,2).contiguous())
       output = F.log_softmax(self.out(output), dim=2)
       return output, hidden.permute(1,0,2)

class AttnDecoderRNN(nn.Module):
    def __init__(self,vocab_size, hidden_size,output_size, dropout_p=0.1, max_length=MAX_LENGTH):
        super(AttnDecoderRNN, self).__init__()
        self.hidden_size = hidden_size
        self.vocab_size=vocab_size
        # self.embedding_size=embedding_size
        self.output_size = output_size
        self.dropout_p = dropout_p
        self.max_length = max_length

        self.embedding = nn.Embedding(self.vocab_size,self.hidden_size)
        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)
        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)
        self.dropout = nn.Dropout(0.1)
        self.gru = nn.GRU(self.hidden_size, self.hidden_size,batch_first=True)
        self.out = nn.Linear(self.hidden_size, self.output_size)

    def forward(self, input, hidden, encoder_outputs):
        # print(input.shape,hidden.shape,encoder_outputs.shape)
        embedded = self.embedding(input).view(encoder_outputs.shape[0], 1, -1)
        embedded = self.dropout(embedded)
        
        hidden=hidden.expand(encoder_outputs.shape[0],1,-1)

        attn_weights = F.softmax(
            self.attn(torch.cat((embedded, hidden), 2)), dim=2)
        # print(attn_weights.shape,encoder_outputs.shape)
        # attn_applied=attn_weights.permute(0,2,1)*encoder_outputs
        attn_applied = torch.bmm(attn_weights,
                                 encoder_outputs)

        output = torch.cat((embedded, attn_applied),dim=2)
        output = self.attn_combine(output)

        output = F.relu(output)
        output, hidden = self.gru(output, hidden.permute(1,0,2).contiguous())
        output = F.log_softmax(self.out(output), dim=2)
        return output, hidden.permute(1,0,2), attn_weights

    def initHidden(self):
        return torch.zeros(1, 1, self.hidden_size, device=device)

# with beam search
import torch.optim as optim
import tqdm
import os
import pickle
from copy import deepcopy
# embedding_size=100
hidden_size=768
# bert_model=BertModel.from_pretrained('bert-base-uncased')
vocab_size=len(tag_index)
device = 'cuda' if torch.cuda.is_available() else 'cpu'
encoder=Encoder().to(device)

# if(os.path.exists('/content/drive/My Drive/BERT-SEQ-Tagger/encoder.pt')):
#     encoder.load_state_dict(torch.load('/content/drive/My Drive/BERT-SEQ-Tagger/encoder.pt'))

decoder=Decoder(vocab_size,hidden_size,len(tag_index)).to(device)
# if(os.path.exists('/content/drive/My Drive/BERT-SEQ-Tagger/decoder.pt')):
#     decoder.load_state_dict(torch.load('/content/drive/My Drive/BERT-SEQ-Tagger/decoder.pt'))

criterion = nn.NLLLoss(ignore_index=tag_index['[PAD]'])
# enc_optimizer = optim.Adam(encoder.parameters(), lr = 2e-5)
dec_optimizer = optim.Adam(decoder.parameters(), lr = 1e-5)

# training_loss=[]
# val_losses=[]
# if(os.path.exists('/content/drive/My Drive/BERT-SEQ-Tagger/val_losses.pkl')):
#   with open('/content/drive/My Drive/BERT-SEQ-Tagger/val_losses.pkl','rb') as f:
#     val_losses=pickle.load(f)

for _e in range(200):
    train_loss=0
    for t, (seq, attn_mask, labels) in enumerate(train_loader):
        # data_batch = sort_batch_by_len(data_dict)
        batch_size=seq.shape[0]
        seq=seq.to(device)
        attn_mask=attn_mask.to(device)
        labels =labels.to(device) #torch.tensor(data_batch).to(device)
                
        # enc_optimizer.zero_grad()
        dec_optimizer.zero_grad()
        encoder_output=encoder(seq,attn_mask)        
        decoder_input = torch.tensor([batch_size*[tag_index['<s>']]], device=device).view(-1,1)
        decoder_hidden=encoder_output.view(batch_size,1,-1)
        labels= torch.cat((labels,torch.tensor(batch_size*[tag_index['</s>']], device=device).view(-1,1)),dim=1)
        # loss=0
        # for di in range(labels.shape[1]):
        #   decoder_output,decoder_hidden=decoder(decoder_hidden,decoder_input)
        #   loss += criterion(decoder_output.view(encoder_output.shape[0],-1), labels[:,di])
        #   train_loss+=loss.data.item()
        #   decoder_input = labels[:,di]

        step_losses=[torch.tensor(0)]
        decoder_inputs=[]
        decoder_inputs.append(decoder_input)
        for di in range(5):
          loss=[]
          decoder_outputs=[]
          for indx,input in enumerate(decoder_inputs):
            for k in range(input.shape[1]):
              # print(decoder_hidden.shape,input[:,k].shape)
              decoder_output,decoder_hidden=decoder(decoder_hidden,input[:,k])
              # input_loss[-1] += criterion(decoder_output.view(encoder_output.shape[0],-1), labels[:,di])
              _, top_idx = decoder_output.data.topk(2)
              decoder_outputs.append(top_idx.view(encoder_output.shape[0],-1))
              # print(decoder_output.shape,labels[:,di].shape)
              loss.append(criterion(decoder_output.view(encoder_output.shape[0],-1),labels[:,di]))            
          step_losses=loss
          decoder_inputs=decoder_outputs
        arg=np.argmin([t.data.item() for t in step_losses])  
        loss=step_losses[arg]
        train_loss+=loss.data.item()
        loss.backward()   
        # enc_optimizer.step()
        dec_optimizer.step()
    # train_loss=train_loss/len(train)
    # print(train_loss)
    # print(train_loss)    
    val_loss=0
    for t, (seq, attn_mask, labels) in enumerate(val_loader):
        seq=seq.to(device)
        attn_mask=attn_mask.to(device)
        labels =labels.to(device) #torch.tensor(data_batch).to(device)
        batch_size=seq.shape[0]
        # enc_optimizer.zero_grad()
        dec_optimizer.zero_grad()
        encoder_output=encoder(seq,attn_mask)
        decoder_input = torch.tensor([batch_size*[tag_index['<s>']]], device=device).view(-1,1)
        decoder_hidden=encoder_output.view(batch_size,1,-1)
        labels= torch.cat((labels,torch.tensor(batch_size*[tag_index['</s>']], device=device).view(-1,1)),dim=1)
        step_losses=[torch.tensor(0)]
        decoder_inputs=[]
        decoder_inputs.append(decoder_input)
        for di in range(labels.shape[1]):
          input_loss=[]
          decoder_outputs=[]
          for indx,input in enumerate(decoder_inputs):
            decoder_output,decoder_hidden=decoder(decoder_hidden,input)
            # input_loss[-1] += criterion(decoder_output.view(encoder_output.shape[0],-1), labels[:,di])
            _, top_idx = decoder_output.data.topk(3)
            decoder_outputs.append([top_idx.view(encoder_output.shape[0],-1)])
            for i in range(top_idx.shape[1]):
              input_loss.append(torch.clone(step_losses[indx]))
              
              input_loss[-1] += criterion(top_idx[:,i], labels[:,di])
          step_losses=deepcopy(input_loss)    
          decoder_inputs=deepcopy(decoder_outputs)
          val_loss+=torch.min(torch.cat(step_losses)).data.item()
    val_loss=val_loss/len(val)
    if(len(val_losses)>0 and val_loss<min(val_losses)):
      torch.save(encoder.state_dict(), '/content/drive/My Drive/BERT-SEQ-Tagger/encoder.pt') 
      torch.save(decoder.state_dict(), '/content/drive/My Drive/BERT-SEQ-Tagger/decoder.pt')  
    val_losses.append(val_loss)      
    print('training loss:{} validation loss:{}'.format(train_loss,val_loss))

