{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/DEEPML/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/anaconda3/envs/DEEPML/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/anaconda3/envs/DEEPML/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/anaconda3/envs/DEEPML/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/anaconda3/envs/DEEPML/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/anaconda3/envs/DEEPML/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/anaconda3/envs/DEEPML/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/anaconda3/envs/DEEPML/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/anaconda3/envs/DEEPML/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/anaconda3/envs/DEEPML/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/anaconda3/envs/DEEPML/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/anaconda3/envs/DEEPML/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import glob2\n",
    "import os\n",
    "import bs4\n",
    "import nltk\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Users/nirmalenduprakash/Documents/Project/NLP/Sentiment Mining')\n",
    "filenames=glob2.glob('upload/database.mpqa.3.0/gate_anns/**/*.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNodeText(text,node1,node2):\n",
    "    text= text[text.index(\"\\\"\"+node1+\"\\\"\"):text.index(\"\\\"\"+node2+\"\\\"\")]\n",
    "    nodes_in_text=re.findall(\"\\\"\\d*\\\"\",text)\n",
    "    for node in nodes_in_text:\n",
    "        text=text.replace(node,'')    \n",
    "    text=text.replace('node','')\\\n",
    "              .replace('>','').replace('<','').replace('id','').replace('=','').replace('\\\"','')\\\n",
    "             .replace('/','')#.replace(node1,'')#.replace(node2,'')\n",
    "\n",
    "    return text    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSentences(text):\n",
    "    return nltk.sent_tokenize(text)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNodeSent(sent,txt_with_nodes):\n",
    "#     print(sent,txt_with_nodes)\n",
    "    result=''\n",
    "    search_text=txt_with_nodes\n",
    "    for word in nltk.word_tokenize(sent):  \n",
    "        try:\n",
    "            result=result+str(search_text[:search_text.index(word) + len(word)])      \n",
    "            search_text=txt_with_nodes[len(result):]\n",
    "        except:\n",
    "            continue        \n",
    "    return result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents=[]\n",
    "for file in filenames:\n",
    "    with open(file,'r') as f:\n",
    "        soup=bs4.BeautifulSoup(f)\n",
    "        txt=soup.select('TextWithNodes')[0].text.replace('\\n','').replace('\\t','')\n",
    "        documents.append(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "tokenizer = BertTokenizer.from_pretrained(os.curdir)\n",
    "seq_train=[]\n",
    "for file in filenames:\n",
    "    with open(file,'r') as f:\n",
    "        soup=bs4.BeautifulSoup(f)\n",
    "        txt=soup.select('TextWithNodes')[0].text\n",
    "        sents=getSentences(txt)\n",
    "        txt_with_nodes=str(soup.select('TextWithNodes')[0])    \n",
    "        agents=[{'startNode':node['startnode'],\\\n",
    "                       'endNode':node['endnode'], \\\n",
    "                       'text':getNodeText(txt_with_nodes,node['startnode'],node['endnode'])}\\\n",
    "                      for node in soup.select(\"Annotation[Type=agent]\")]\n",
    "        sentiments=[{'startNode':node['startnode'],\\\n",
    "                       'endNode':node['endnode'], \\\n",
    "                       'intensity':str(list(list(node.children)[3])[3]).split('>')[1].split('<')[0],\\\n",
    "                       'type':str(list(list(node.children)[5])[3]).split('>')[1].split('<')[0]}\\\n",
    "                      for node in soup.select(\"Annotation[Type=attitude]\")]\n",
    "        targets=[{'startNode':node['startnode'],\\\n",
    "                       'endNode':node['endnode'], \\\n",
    "                       'text':getNodeText(txt_with_nodes,node['startnode'],node['endnode'])}\\\n",
    "                      for node in soup.select(\"Annotation[Type=sTarget]\")]\n",
    "        for indx,sent in enumerate(sents):\n",
    "            nodeText=getNodeSent(sent,txt_with_nodes)        \n",
    "            nodes_in_txt=re.findall('\\d+',nodeText)\n",
    "            sent_agent=None\n",
    "            sent_target=None\n",
    "            sent_sentiment_type=None\n",
    "            sent_sentiment_intensity=None\n",
    "            \n",
    "            for node in nodes_in_txt:\n",
    "                sent_agents=[itm['text'] for itm in agents if itm['startNode']==node]\n",
    "                if(len(sent_agents)>0):\n",
    "                    sent_agent=sent_agents[0].replace('  ',' ')\n",
    "                sent_targets=[itm['text'] for itm in targets if itm['startNode']==node]\n",
    "                if(len(sent_targets)>0):\n",
    "                    sent_target=sent_targets[0].replace('  ',' ') \n",
    "                sent_intensities=[itm['intensity'] for itm in sentiments if itm['startNode']==node]\n",
    "                if(len(sent_intensities)>0):\n",
    "                    sent_sentiment_intensity=sent_intensities[0]\n",
    "                sent_types=[itm['type'] for itm in sentiments if itm['startNode']==node]\n",
    "                if(len(sent_types)>0):\n",
    "                    sent_sentiment_type=sent_types[0]     \n",
    "#                 print('sent:{}'.format(sent),'agent:{}'.format(sent_agent),'target:{}'.format(sent_target),\\\n",
    "#                       'sentiment type:{}'.format(sent_sentiment_type),\\\n",
    "#                       'sentiment intensity:{}'.format(sent_sentiment_intensity))\n",
    "            \n",
    "            txt_with_nodes=txt_with_nodes[len(nodeText):]\n",
    "            labels=tokenizer.tokenize(sent)\n",
    "            sent=tokenizer.tokenize(sent)\n",
    "            \n",
    "            if(sent_agent is not None):\n",
    "                n=len(tokenizer.tokenize(sent_agent))\n",
    "                gms=nltk.ngrams(labels,n)\n",
    "                for i,g in enumerate(gms):\n",
    "                    if(' '.join(g).strip()==' '.join(tokenizer.tokenize(sent_agent)).strip()):\n",
    "                        labels[i:i+len(g)]=['B-AG' if j==0 else 'I-AG' for j in range(len(g))]\n",
    "                        break\n",
    "                        \n",
    "            if(sent_target is not None and (sent_agent is not None and sent_agent.strip()!=sent_target.strip())):\n",
    "                n=len(tokenizer.tokenize(sent_target))\n",
    "                gms=nltk.ngrams(labels,n)\n",
    "                for i,g in enumerate(gms):\n",
    "                    if(' '.join(g).strip()==' '.join(tokenizer.tokenize(sent_target)).strip()):\n",
    "                        labels[i:i+len(g)]=['B-TG' if j==0 else 'I-TG' for j in range(len(g))]\n",
    "                        break            \n",
    "                 \n",
    "            labels=['O' if w not in ['B-AG','I-AG','B-TG','I-TG'] else w for w in labels]  \n",
    "            seq_train.append({'sent':sent,'labels':labels,'sentiment_type':sent_sentiment_type,\\\n",
    "                             'sentiment_intensity':sent_sentiment_intensity,\\\n",
    "                              'file_name':file,'agent':sent_agent,'target':sent_target})\n",
    "#             print(seq_train)\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent</th>\n",
       "      <th>labels</th>\n",
       "      <th>sentiment_type</th>\n",
       "      <th>sentiment_intensity</th>\n",
       "      <th>file_name</th>\n",
       "      <th>agent</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[following, the, end, of, the, cold, war, ,, t...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, B-AG, I-AG, I-AG, O, ...</td>\n",
       "      <td>arguing-pos</td>\n",
       "      <td>medium</td>\n",
       "      <td>upload/database.mpqa.3.0/gate_anns/20011208/20...</td>\n",
       "      <td>the United States</td>\n",
       "      <td>the trend of pragmatism in its foreign policy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[pr, ##ag, ##mat, ##ism, is, ,, in, the, first...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>arguing-pos</td>\n",
       "      <td>low-medium</td>\n",
       "      <td>upload/database.mpqa.3.0/gate_anns/20011208/20...</td>\n",
       "      <td>None</td>\n",
       "      <td>Pragmatism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[after, coming, to, power, ,, un, ##ila, ##ter...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>somewhat-uncertain</td>\n",
       "      <td>a100</td>\n",
       "      <td>upload/database.mpqa.3.0/gate_anns/20011208/20...</td>\n",
       "      <td>Bush</td>\n",
       "      <td>Bush</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[alternate, use, of, and, complementary, natur...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>arguing-pos</td>\n",
       "      <td>low-medium</td>\n",
       "      <td>upload/database.mpqa.3.0/gate_anns/20011208/20...</td>\n",
       "      <td>None</td>\n",
       "      <td>Alternate use of and complementary nature of p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[once, coming, to, power, ,, bush, insisted, o...</td>\n",
       "      <td>[O, O, O, O, O, B-AG, O, O, B-TG, I-TG, I-TG, ...</td>\n",
       "      <td>sentiment-pos</td>\n",
       "      <td>low-medium</td>\n",
       "      <td>upload/database.mpqa.3.0/gate_anns/20011208/20...</td>\n",
       "      <td>Bush</td>\n",
       "      <td>deploying the National Missile Defense System</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sent  \\\n",
       "0  [following, the, end, of, the, cold, war, ,, t...   \n",
       "1  [pr, ##ag, ##mat, ##ism, is, ,, in, the, first...   \n",
       "2  [after, coming, to, power, ,, un, ##ila, ##ter...   \n",
       "3  [alternate, use, of, and, complementary, natur...   \n",
       "4  [once, coming, to, power, ,, bush, insisted, o...   \n",
       "\n",
       "                                              labels      sentiment_type  \\\n",
       "0  [O, O, O, O, O, O, O, O, B-AG, I-AG, I-AG, O, ...         arguing-pos   \n",
       "1  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...         arguing-pos   \n",
       "2  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  somewhat-uncertain   \n",
       "3  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...         arguing-pos   \n",
       "4  [O, O, O, O, O, B-AG, O, O, B-TG, I-TG, I-TG, ...       sentiment-pos   \n",
       "\n",
       "  sentiment_intensity                                          file_name  \\\n",
       "0              medium  upload/database.mpqa.3.0/gate_anns/20011208/20...   \n",
       "1          low-medium  upload/database.mpqa.3.0/gate_anns/20011208/20...   \n",
       "2                a100  upload/database.mpqa.3.0/gate_anns/20011208/20...   \n",
       "3          low-medium  upload/database.mpqa.3.0/gate_anns/20011208/20...   \n",
       "4          low-medium  upload/database.mpqa.3.0/gate_anns/20011208/20...   \n",
       "\n",
       "                agent                                             target  \n",
       "0  the United States      the trend of pragmatism in its foreign policy   \n",
       "1                None                                        Pragmatism   \n",
       "2               Bush                                               Bush   \n",
       "3                None  Alternate use of and complementary nature of p...  \n",
       "4               Bush      deploying the National Missile Defense System   "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.DataFrame(seq_train)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "max_len_lbl=np.max([df['labels'].apply(lambda x: len(x))])\n",
    "max_len_sent=np.max([df['sent'].apply(lambda x: len(x))])\n",
    "# df['sent']=df['sent'].apply(lambda x:['[CLS]'+x+'[SEP]'])\n",
    "# df['labels']=df['labels'].apply(lambda x:['[CLS]'+x+'[SEP]'])\n",
    "max_len=max(max_len_lbl,max_len_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(tokens,max_len,cls=False):\n",
    "  if(len(tokens)<max_len):\n",
    "    if(cls):\n",
    "        return ['[CLS]']+tokens+['[PAD]' for _ in range(max_len-len(tokens)-1)]\n",
    "    else:\n",
    "        return tokens+['[PAD]' for _ in range(max_len-len(tokens))]\n",
    "  else:\n",
    "    return tokens[:max_len] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['labels']=df['labels'].apply(lambda x: pad([l for l in x],max_len))\n",
    "df['sent']=df['sent'].apply(lambda x: pad([l for l in x],max_len,True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['encoding']=df['sent'].apply(lambda x: tokenizer.convert_tokens_to_ids(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['attn_mask']=df['encoding'].apply(lambda x: [0 if tok==str('[PAD]') else 1 for tok in x ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment_intensity']=df['sentiment_intensity'].apply(lambda x: x if x in ['medium', 'low-medium',\\\n",
    "                                                        'medium-high', 'low', 'high-extreme', 'high'] else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment_type']=df['sentiment_type'].apply(lambda x: x if x in ['arguing-pos', 'somewhat-uncertain', \n",
    "       'sentiment-pos','intention-pos', 'sentiment-neg', 'other-attitude',\n",
    "       'very-uncertain', 'arguing-neg', 'intention-neg', 'agree-pos','agree-neg'] else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1705\n",
      "sent                     0\n",
      "labels                   0\n",
      "sentiment_type         763\n",
      "sentiment_intensity    852\n",
      "file_name                0\n",
      "agent                  812\n",
      "target                 779\n",
      "encoding                 0\n",
      "attn_mask                0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(len(df))\n",
    "print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "783\n"
     ]
    }
   ],
   "source": [
    "# df['agent'].fillna('author',inplace=True)\n",
    "df.dropna(inplace=True)\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('/Users/nirmalenduprakash/Documents/Project/NLP/Sentiment Mining/documents.pkl','wb') as f:\n",
    "    pickle.dump(documents,f)\n",
    "with open('/Users/nirmalenduprakash/Documents/Project/NLP/Sentiment Mining/data.pkl','wb') as f:\n",
    "    pickle.dump(df,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/nirmalenduprakash/Documents/Project/NLP/Sentiment Mining/sample_document.txt','w') as f:\n",
    "    f.write(documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('DEEPML': conda)",
   "language": "python",
   "name": "python37664bitdeepmlcondaa8309bcee70a4550b3efd0d3a9e3abda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
