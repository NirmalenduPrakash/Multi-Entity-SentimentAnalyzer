{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Subject-Object-BERT-SEQ.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "GkgENdIIrjs3",
        "colab_type": "code",
        "outputId": "11d84929-7514-474c-e314-5890a2ecb493",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7Jels3nr6ek",
        "colab_type": "code",
        "outputId": "d585b21a-30ec-482a-cd65-df2424594ddd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 712
        }
      },
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "!pip install transformers\n",
        "from transformers import BertTokenizer\n",
        "# from keras.preprocessing.sequence import pad_sequences\n",
        "from torch.nn import functional as F"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/a0/32e3a4501ef480f7ea01aac329a716132f32f7911ef1c2fac228acc57ca7/transformers-2.6.0-py3-none-any.whl (540kB)\n",
            "\u001b[K     |████████████████████████████████| 542kB 6.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.12.26)\n",
            "Collecting tokenizers==0.5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7MB 41.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.2)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 46.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 42.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.26 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.15.26)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.26->boto3->transformers) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.26->boto3->transformers) (0.15.2)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884628 sha256=549e696df39ae9c43703b2c9f14cb9686f469f4f01781582072f942a76a1308b\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, sentencepiece, transformers\n",
            "Successfully installed sacremoses-0.0.38 sentencepiece-0.1.85 tokenizers-0.5.2 transformers-2.6.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will switch to TensorFlow 2.x on the 27th of March, 2020.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now\n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbtfGyyDsBo0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "with open('/content/drive/My Drive/data.pkl','rb') as f:\n",
        "  df=pickle.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhPwMituNhlm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_LENGTH=len(df.iloc[0]['encoding'])\n",
        "tag_index={'[PAD]':0,'<s>':1,'B-AG':2,'I-AG':3,'B-TG':4,'I-TG':5,'O':6,'</s>':7}\n",
        "df['labels']=df['labels'].apply(lambda x: [tag_index[lbl] for lbl in x])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FiefdIZVtE-R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import Dataset\n",
        "class SequenceDataset(Dataset):\n",
        "  def __init__(self,df):\n",
        "    self.df=df\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.df)\n",
        "\n",
        "  def __getitem__(self,index):\n",
        "    return torch.tensor(self.df.iloc[index]['encoding']),torch.tensor(self.df.iloc[index]['attn_mask']),torch.tensor(self.df.iloc[index]['labels'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRmU_nsgJuuy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "msk = np.random.rand(len(df)) < 0.8\n",
        "train=df[msk]\n",
        "val=df[~msk]\n",
        "train_set=SequenceDataset(train)\n",
        "val_set=SequenceDataset(val)\n",
        "train_loader=DataLoader(train_set, batch_size = 2)\n",
        "val_loader = DataLoader(val_set, batch_size = 2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYIsNn8sJ45L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "from transformers import BertModel\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, freeze_bert = True):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.bert_layer = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "        # for p in self.bert_layer.parameters():\n",
        "        #     p.requires_grad = False\n",
        "\n",
        "    def forward(self, seq, attn_masks):\n",
        "        cont_reps, _ = self.bert_layer(seq, attention_mask = attn_masks)\n",
        "        return cont_reps[:,0]        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqgknxMKZmhs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self,vocab_size, hidden_size,output_size, dropout_p=0.1):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.vocab_size=vocab_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.embedding = nn.Embedding(self.vocab_size,self.hidden_size)\n",
        "        self.gru = nn.GRU(self.hidden_size, self.hidden_size,batch_first=True)\n",
        "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "    def forward(self, hidden,input):\n",
        "       embedded = self.embedding(input).view(hidden.shape[0], 1, -1)\n",
        "       embedded = self.dropout(embedded)\n",
        "       output, hidden = self.gru(embedded, hidden.permute(1,0,2).contiguous())\n",
        "       output = F.log_softmax(self.out(output), dim=2)\n",
        "       return output, hidden.permute(1,0,2)   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPIM4StnMPK0",
        "colab_type": "code",
        "outputId": "16a513e3-108c-4061-cee1-4f025109bbc3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        }
      },
      "source": [
        "import torch.optim as optim\n",
        "import tqdm\n",
        "import os\n",
        "import pickle\n",
        "# embedding_size=100\n",
        "hidden_size=768\n",
        "# bert_model=BertModel.from_pretrained('bert-base-uncased')\n",
        "vocab_size=len(tag_index)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "encoder=Encoder().to(device)\n",
        "\n",
        "if(os.path.exists('/content/drive/My Drive/BERT-SEQ-Tagger/encoder.pt')):\n",
        "    encoder.load_state_dict(torch.load('/content/drive/My Drive/BERT-SEQ-Tagger/encoder.pt'))\n",
        "\n",
        "decoder=Decoder(vocab_size,hidden_size,len(tag_index)).to(device)\n",
        "if(os.path.exists('/content/drive/My Drive/BERT-SEQ-Tagger/decoder.pt')):\n",
        "    decoder.load_state_dict(torch.load('/content/drive/My Drive/BERT-SEQ-Tagger/decoder.pt'))\n",
        "\n",
        "criterion = nn.NLLLoss(ignore_index=tag_index['[PAD]'])\n",
        "# enc_optimizer = optim.Adam(encoder.parameters(), lr = 2e-5)\n",
        "dec_optimizer = optim.Adam(decoder.parameters(), lr = 2e-5)\n",
        "\n",
        "# training_loss=[]\n",
        "val_losses=[]\n",
        "if(os.path.exists('/content/drive/My Drive/BERT-SEQ-Tagger/val_losses.pkl')):\n",
        "  with open('/content/drive/My Drive/BERT-SEQ-Tagger/val_losses.pkl','rb') as f:\n",
        "    val_losses=pickle.load(f)\n",
        "\n",
        "for _e in range(200):\n",
        "    train_loss=0\n",
        "    for t, (seq, attn_mask, labels) in enumerate(train_loader):\n",
        "        # data_batch = sort_batch_by_len(data_dict)\n",
        "        batch_size=seq.shape[0]\n",
        "        seq=seq.to(device)\n",
        "        attn_mask=attn_mask.to(device)\n",
        "        labels =labels.to(device) #torch.tensor(data_batch).to(device)\n",
        "                \n",
        "        # enc_optimizer.zero_grad()\n",
        "        dec_optimizer.zero_grad()\n",
        "        encoder_output=encoder(seq,attn_mask)        \n",
        "        decoder_input = torch.tensor([batch_size*[tag_index['<s>']]], device=device).view(-1,1)\n",
        "        decoder_hidden=encoder_output.view(batch_size,1,-1)\n",
        "        labels= torch.cat((labels,torch.tensor(batch_size*[tag_index['</s>']], device=device).view(-1,1)),dim=1)\n",
        "        loss=0\n",
        "        for di in range(labels.shape[1]):\n",
        "          decoder_output,decoder_hidden=decoder(decoder_hidden,decoder_input)\n",
        "          # print(decoder_output.squeeze(0).shape)\n",
        "          loss += criterion(decoder_output.view(encoder_output.shape[0],-1), labels[:,di])\n",
        "          train_loss+=loss.data.item()\n",
        "          decoder_input = labels[:,di]\n",
        "        loss.backward()   \n",
        "        # enc_optimizer.step()\n",
        "        dec_optimizer.step()\n",
        "    train_loss=train_loss/len(train)\n",
        "    # print(train_loss)    \n",
        "    val_loss=0\n",
        "    for t, (seq, attn_mask, labels) in enumerate(val_loader):\n",
        "        seq=seq.to(device)\n",
        "        attn_mask=attn_mask.to(device)\n",
        "        labels =labels.to(device) #torch.tensor(data_batch).to(device)\n",
        "        batch_size=seq.shape[0]\n",
        "        # enc_optimizer.zero_grad()\n",
        "        dec_optimizer.zero_grad()\n",
        "        encoder_output=encoder(seq,attn_mask)\n",
        "        decoder_input = torch.tensor([batch_size*[tag_index['<s>']]], device=device).view(-1,1)\n",
        "        decoder_hidden=encoder_output.view(batch_size,1,-1)\n",
        "        labels= torch.cat((labels,torch.tensor(batch_size*[tag_index['</s>']], device=device).view(-1,1)),dim=1)\n",
        "        loss=0\n",
        "        for di in range(labels.shape[1]):\n",
        "          decoder_output,decoder_hidden=decoder(decoder_hidden,decoder_input)\n",
        "          # print(decoder_output.squeeze(0).shape)\n",
        "          loss += criterion(decoder_output.view(encoder_output.shape[0],-1), labels[:,di])\n",
        "          decoder_input = labels[:,di]\n",
        "          # _, top_idx = decoder_output.data.topk(1)\n",
        "          # decoder_input = top_idx.view(-1)\n",
        "          val_loss+=loss.data.item()\n",
        "    val_loss=val_loss/len(val)\n",
        "    if(len(val_losses)>0 and val_loss<min(val_losses)):\n",
        "      torch.save(encoder.state_dict(), '/content/drive/My Drive/BERT-SEQ-Tagger/encoder.pt') \n",
        "      torch.save(decoder.state_dict(), '/content/drive/My Drive/BERT-SEQ-Tagger/decoder.pt')  \n",
        "    val_losses.append(val_loss)      \n",
        "    print('training loss:{} validation loss:{}'.format(train_loss,val_loss))       "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training loss:356.19106928213375 validation loss:392.052931886267\n",
            "training loss:354.71986450172784 validation loss:392.6133257577548\n",
            "training loss:354.022231712867 validation loss:392.12853471572333\n",
            "training loss:353.2969987895545 validation loss:391.92049436875294\n",
            "training loss:352.2381025736726 validation loss:393.54688311187\n",
            "training loss:351.4868782266857 validation loss:392.4445551754655\n",
            "training loss:351.39680223953064 validation loss:392.4150961113943\n",
            "training loss:350.98593977154707 validation loss:392.18388430250656\n",
            "training loss:350.489466122004 validation loss:390.9464576534323\n",
            "training loss:350.08104702318747 validation loss:392.9434012239044\n",
            "training loss:349.7249166811545 validation loss:392.30960919405965\n",
            "training loss:349.28548282018795 validation loss:390.9471149605674\n",
            "training loss:348.68554939454 validation loss:393.1141022556537\n",
            "training loss:348.21755079772527 validation loss:392.4692919528162\n",
            "training loss:348.3713643511449 validation loss:392.43028806754063\n",
            "training loss:347.03234645494325 validation loss:392.92534557468184\n",
            "training loss:347.1862997725254 validation loss:394.0375901549249\n",
            "training loss:346.96784679870905 validation loss:391.2828324388813\n",
            "training loss:346.3271089634557 validation loss:392.5900776821214\n",
            "training loss:346.0660311481145 validation loss:391.23021593770466\n",
            "training loss:345.49790329557703 validation loss:391.61779583547565\n",
            "training loss:345.108611284654 validation loss:392.2419794717351\n",
            "training loss:345.0273183052934 validation loss:392.55907728704244\n",
            "training loss:344.73916636846195 validation loss:394.0314370997854\n",
            "training loss:344.5471666495631 validation loss:392.4617296449236\n",
            "training loss:344.12646236870233 validation loss:392.98819817723455\n",
            "training loss:343.6219091578731 validation loss:390.9277703407648\n",
            "training loss:343.42556042126785 validation loss:391.4823337189249\n",
            "training loss:343.49390648500184 validation loss:393.1338362838771\n",
            "training loss:342.4735161753151 validation loss:392.6133705474235\n",
            "training loss:342.4477812461027 validation loss:392.8622985731911\n",
            "training loss:341.51751481191377 validation loss:393.2378452210813\n",
            "training loss:342.0296047706304 validation loss:391.4574423794811\n",
            "training loss:341.065172308261 validation loss:392.1964914605424\n",
            "training loss:341.35791286896534 validation loss:393.36554095954506\n",
            "training loss:340.9269343190306 validation loss:392.7021674111083\n",
            "training loss:340.2150380389897 validation loss:392.8005644397156\n",
            "training loss:340.2605527491081 validation loss:394.9178142708701\n",
            "training loss:339.84084089647126 validation loss:392.629766436042\n",
            "training loss:339.44983964905026 validation loss:394.97003000008095\n",
            "training loss:339.1254320262924 validation loss:396.56351304779184\n",
            "training loss:338.8943685163663 validation loss:393.3420517057986\n",
            "training loss:338.6809878167205 validation loss:394.41708372251406\n",
            "training loss:338.4018594503403 validation loss:397.1239779962076\n",
            "training loss:338.2941137223732 validation loss:394.04923556221496\n",
            "training loss:338.12622908156686 validation loss:394.9237692323891\n",
            "training loss:337.2968680466254 validation loss:395.5418545821229\n",
            "training loss:337.1953340250676 validation loss:393.5340242112005\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQ72t0kNfeDW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "with open('/content/drive/My Drive/BERT-SEQ-Tagger/val_losses.pkl','wb') as f:\n",
        "  pickle.dump(val_losses,f)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}